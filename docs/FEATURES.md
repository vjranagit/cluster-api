# New Features Documentation

This fork includes three major production-ready features that address common pain points in cluster lifecycle management.

## 1. Drift Detection & Auto-Remediation

### Overview
Drift detection automatically identifies when actual infrastructure state diverges from the declared configuration, enabling:
- Early detection of unauthorized changes
- Automated remediation of configuration drift
- Compliance enforcement
- Continuous validation of infrastructure state

### Architecture
The drift detector compares the desired state (from HCL configuration) against actual cloud provider state, categorizing differences by severity and remediability.

```go
type DriftDetector struct {
    engine *engine.Engine
    logger *slog.Logger
}
```

### Drift Types
- **config_change**: Manual configuration modifications
- **version_skew**: Kubernetes version mismatches
- **scale_change**: Node count modifications
- **network_change**: Network configuration changes
- **security_change**: Security group/IAM modifications
- **resource_deleted**: Resources removed externally
- **resource_added**: Unexpected resources exist

### Severity Levels
- **Critical**: Immediate action required (e.g., cluster deleted)
- **High**: Should remediate soon (e.g., version skew)
- **Medium**: Non-urgent drift (e.g., scale changes)
- **Low**: Informational only

### Usage

#### Detect Drift
```bash
provctl drift detect cluster.hcl
```

Output:
```
âš  Drift Detected at 2024-02-03T03:30:00Z

Summary: 3 total drifts (1 critical, 1 high, 1 medium, 0 low)
Remediatable: 3

Detected Drifts:
  ðŸ”´ Cluster/production - resource_deleted [auto-fixable]
      Field: cluster
      Expected: exists
      Actual: deleted

  ðŸŸ  Cluster/staging - version_skew [auto-fixable]
      Field: controlPlane.version
      Expected: 1.29
      Actual: 1.28

  ðŸŸ¡ NodePool/general - scale_change [auto-fixable]
      Field: desiredSize
      Expected: 5
      Actual: 3
```

#### Auto-Remediate
```bash
provctl drift remediate cluster.hcl
```

Or enable continuous drift detection:
```bash
provctl drift watch --interval=5m --auto-remediate cluster.hcl
```

### Implementation Details

**Detection Algorithm:**
1. Load desired state from HCL configuration
2. Query actual state from cloud providers (AWS/Azure)
3. Deep-compare resources (clusters, node pools, networks)
4. Categorize differences by type and severity
5. Generate remediation plan for fixable drift

**Remediation Process:**
1. Validate drift is remediatable
2. Create snapshot before remediation
3. Apply corrective actions via provider APIs
4. Verify remediation success
5. Record event in audit log

### Configuration

Enable automatic drift detection in your cluster spec:
```hcl
cluster "production" {
  provider = "aws"
  region   = "us-west-2"

  drift_detection {
    enabled           = true
    check_interval    = "5m"
    auto_remediate    = true
    ignore_fields     = ["worker_pools.*.desired_size"]  # Allow autoscaler changes
  }
}
```

### Best Practices
1. **Start with detection only** - Monitor drift patterns before enabling auto-remediation
2. **Ignore autoscaler fields** - Don't remediate changes made by cluster autoscaler
3. **Review critical drift manually** - Set auto-remediation to high/medium severity only
4. **Enable audit logging** - Track all drift events for compliance
5. **Set up alerts** - Integrate with monitoring to alert on critical drift

---

## 2. Cost Estimation Engine

### Overview
The cost estimator calculates projected monthly and hourly infrastructure costs before applying changes, preventing budget surprises and enabling cost-conscious decisions.

### Features
- **Pre-deployment cost analysis** - Know costs before provisioning
- **Multi-cloud pricing** - Supports AWS and Azure pricing models
- **Spot savings calculations** - Identifies potential savings
- **Detailed breakdowns** - Per-resource cost analysis
- **Cost warnings** - Alerts for high-cost configurations
- **Optimization recommendations** - Suggests cost-saving opportunities

### Architecture
```go
type Estimator struct {
    pricingData map[string]PricingData
}

type CostEstimate struct {
    TotalMonthlyCost float64
    TotalHourlyCost  float64
    Breakdown        []CostBreakdown
    Warnings         []string
}
```

### Usage

#### Estimate Costs
```bash
provctl cost estimate cluster.hcl
```

Output:
```
ðŸ’° Cost Estimate (generated 2024-02-03 03:30:00)

Total Monthly Cost: $487.60
Total Hourly Cost:  $0.6680

Breakdown by Resource:
  â€¢ ControlPlane/managed-control-plane: $73.00/month
    Managed K8s control plane (1.28) ($0.1000/hour x 1)

  â€¢ NodePool/general: $243.12/month
    5 x t3.medium (on-demand) ($0.0416/hour x 5)

  â€¢ NodePool/compute: $111.66/month
    3 x c5.xlarge (spot) ($0.0510/hour x 3)

  â€¢ Network/nat-gateways: $32.85/month
    1 NAT Gateway(s) ($0.0450/hour x 1)

  â€¢ Network/load-balancer: $18.25/month
    Network Load Balancer ($0.0250/hour x 1)

Breakdown by Type:
  â€¢ managed_k8s: $73.00/month (15.0%)
  â€¢ compute: $354.78/month (72.8%)
  â€¢ network: $51.10/month (10.5%)

Assumptions:
  â€¢ Assumes 730 hours per month (24/7 operation)
  â€¢ Prices based on latest public pricing data
  â€¢ Does not include data transfer or storage costs

Warnings & Recommendations:
  ðŸ’¡ Potential savings of $87.24/month by using spot instances
```

#### Compare Configuration Changes
```bash
provctl cost diff current.hcl proposed.hcl
```

Output:
```
Cost Comparison:
  Current:  $487.60/month
  Proposed: $312.45/month
  Savings:  $175.15/month (35.9%)

Changes:
  â€¢ Migrated "general" pool to spot instances
  â€¢ Reduced NAT gateway count from 3 to 1
```

### Supported Cost Categories

**Compute:**
- EC2 instances (on-demand & spot)
- Azure VMs (standard & spot)
- Auto-scaling groups
- Instance types across all regions

**Managed Kubernetes:**
- EKS control plane ($0.10/hour)
- AKS control plane (free)
- Per-node fees (provider-specific)

**Networking:**
- NAT Gateways
- Load Balancers (NLB, ALB, Azure LB)
- VPCs/VNets
- Data transfer (estimated)

**Storage:**
- EBS volumes (gp3, io1, io2)
- Azure Managed Disks
- IOPS provisioning

### Pricing Data
Pricing data is loaded from embedded tables based on latest public cloud pricing:

```go
// AWS US-West-2 Pricing
"t3.medium": {OnDemandHourly: 0.0416, SpotHourly: 0.0125}
"t3.large":  {OnDemandHourly: 0.0832, SpotHourly: 0.0250}
"c5.xlarge": {OnDemandHourly: 0.170,  SpotHourly: 0.0510}

// Azure East US Pricing
"Standard_D2s_v3": {OnDemandHourly: 0.096, SpotHourly: 0.0288}
"Standard_D4s_v3": {OnDemandHourly: 0.192, SpotHourly: 0.0576}
```

Pricing data is refreshed periodically and can be customized per organization.

### Cost Optimization Features

**Automatic Savings Detection:**
- Identifies pools that could use spot instances
- Recommends reserved instance opportunities
- Flags over-provisioned resources

**Budget Alerts:**
```hcl
cluster "production" {
  cost_budget {
    monthly_limit     = 1000.0  # USD
    alert_threshold   = 0.8     # Alert at 80%
    enforce_limit     = false   # Don't block, just warn
  }
}
```

### Integration with CI/CD
```yaml
# GitHub Actions example
- name: Estimate Costs
  run: |
    provctl cost estimate cluster.hcl > cost-estimate.txt
    COST=$(grep "Total Monthly Cost" cost-estimate.txt | awk '{print $4}')
    if (( $(echo "$COST > 500" | bc -l) )); then
      echo "::warning::Monthly cost exceeds $500: $COST"
    fi
```

---

## 3. Snapshot & Rollback

### Overview
State snapshots provide point-in-time backups of infrastructure state with fast rollback capabilities, enabling safe operations and disaster recovery.

### Features
- **Automatic pre-change snapshots** - Created before upgrades/deletes
- **Manual snapshots** - On-demand backups
- **Fast rollback** - Restore to any previous state
- **Integrity verification** - Checksums prevent corruption
- **Retention policies** - Automatic pruning of old snapshots
- **Dry-run restore** - Preview changes before applying

### Architecture
```go
type Snapshot struct {
    ID          string
    CreatedAt   time.Time
    Description string
    State       engine.State
    Metadata    SnapshotMetadata
    Checksum    string
}
```

### Usage

#### Create Manual Snapshot
```bash
provctl snapshot create --description "Before major upgrade"
```

Output:
```
ðŸ“¸ Snapshot created: snapshot-20240203-033000
Clusters: 3
Node Pools: 8
Size: 45.2 KB
```

#### List Snapshots
```bash
provctl snapshot list
```

Output:
```
Snapshots:
ID                          Created              Trigger      Clusters  Pools  Size
snapshot-20240203-033000    2024-02-03 03:30:00  manual       3         8      45.2KB
snapshot-20240203-020000    2024-02-03 02:00:00  pre_upgrade  3         8      44.8KB
snapshot-20240202-180000    2024-02-02 18:00:00  scheduled    2         6      32.1KB
```

#### Restore Snapshot (Dry Run)
```bash
provctl snapshot restore snapshot-20240203-020000 --dry-run
```

Output:
```
ðŸ“¸ Snapshot Restore snapshot-20240203-020000

âš  DRY RUN - No changes were applied

Changes: 1 to add, 2 to modify, 0 to remove

Detailed Changes:
  + Cluster/staging
  ~ Cluster/production
  ~ NodePool/general
```

#### Restore Snapshot
```bash
provctl snapshot restore snapshot-20240203-020000
```

Output:
```
ðŸ“¸ Snapshot Restore snapshot-20240203-020000

âœ“ Restore completed successfully
Backup created: snapshot-20240203-033015

Changes: 1 to add, 2 to modify, 0 to remove

Detailed Changes:
  + Cluster/staging
  ~ Cluster/production
  ~ NodePool/general
```

### Automatic Snapshots

Snapshots are automatically created for:
- **Pre-Upgrade** - Before Kubernetes version upgrades
- **Pre-Delete** - Before deleting clusters
- **Pre-Apply** - Before applying significant changes
- **Drift Remediation** - Before auto-remediation
- **Scheduled** - Periodic backups

Configure automatic snapshots:
```hcl
cluster "production" {
  snapshots {
    auto_create        = true
    before_upgrade     = true
    before_delete      = true
    scheduled_interval = "24h"
    
    retention {
      max_count = 30        # Keep last 30 snapshots
      max_age   = "90d"     # Delete snapshots older than 90 days
    }
  }
}
```

### Snapshot Storage

Snapshots are stored as JSON files in the snapshot directory:
```
~/.provctl/snapshots/
  â”œâ”€â”€ snapshot-20240203-033000.json
  â”œâ”€â”€ snapshot-20240203-020000.json
  â””â”€â”€ snapshot-20240202-180000.json
```

Snapshots can be backed up to:
- S3/Azure Blob Storage
- Git repositories
- Network file systems

### Integrity Verification

Each snapshot includes a checksum:
```bash
provctl snapshot verify snapshot-20240203-033000
```

Output:
```
âœ“ Snapshot integrity verified
Checksum: a3f2c8e9d1b4...
State: Valid
Size: 45.2 KB
```

### Rollback Scenarios

**Failed Upgrade:**
```bash
# Upgrade fails
provctl apply cluster.hcl  # Error during upgrade

# Rollback
provctl snapshot restore $(provctl snapshot list --trigger=pre_upgrade --latest)
```

**Accidental Deletion:**
```bash
# Cluster accidentally deleted
provctl delete production

# Restore from snapshot
provctl snapshot restore --latest
```

**Configuration Mistakes:**
```bash
# Applied wrong configuration
provctl apply wrong-config.hcl

# Rollback to last good state
provctl snapshot restore --latest
```

### Retention Policies

Automatic cleanup of old snapshots:
```bash
# Prune snapshots older than 30 days
provctl snapshot prune --max-age=30d

# Keep only last 20 snapshots
provctl snapshot prune --max-count=20
```

### Best Practices

1. **Enable automatic snapshots** - Always create snapshots before risky operations
2. **Regular scheduled backups** - Daily snapshots for production clusters
3. **Test restore process** - Periodically test snapshot restoration
4. **External backup** - Copy snapshots to external storage
5. **Tag snapshots** - Use descriptive names for manual snapshots
6. **Verify integrity** - Run checksum verification regularly
7. **Retention balance** - Keep enough history without excessive storage

---

## Integration Example

All three features work together seamlessly:

```bash
# 1. Estimate costs before creating cluster
provctl cost estimate production.hcl

# 2. Create initial snapshot
provctl snapshot create --description "Pre-production deployment"

# 3. Apply configuration
provctl apply production.hcl

# 4. Enable drift detection
provctl drift watch --auto-remediate production.hcl &

# 5. Monitor costs and drift over time
provctl cost monitor &

# 6. Automatic snapshots before any changes
# (handled automatically)
```

---

## Production Readiness

All features include:
- âœ… Comprehensive unit tests
- âœ… Error handling and recovery
- âœ… Structured logging (slog)
- âœ… Metrics and observability hooks
- âœ… Documentation and examples
- âœ… CLI integration
- âœ… Multi-cloud support (AWS & Azure)

## Performance Characteristics

**Drift Detection:**
- Scan time: ~2-5 seconds per cluster
- Memory: ~10MB baseline + ~1MB per cluster
- Network: Minimal (uses provider APIs)

**Cost Estimation:**
- Calculation time: <100ms per cluster
- Memory: ~5MB for pricing database
- No network calls (embedded pricing data)

**Snapshots:**
- Create time: <500ms per cluster
- Restore time: <2 seconds per cluster
- Storage: ~10-50KB per cluster snapshot
- No network calls (local storage)

## Future Enhancements

**Drift Detection:**
- GitOps integration for drift remediation via PRs
- Custom drift detection rules
- Drift prediction based on historical patterns

**Cost Estimation:**
- Real-time pricing API integration
- Reserved instance recommendations
- Multi-year cost projections
- Cost allocation by team/project

**Snapshots:**
- Incremental snapshots for large clusters
- Cross-region snapshot replication
- Automated disaster recovery workflows
- Snapshot encryption
